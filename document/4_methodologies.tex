\chapter{Methodologies}
\label{cha:methodologies}


This chapter explains the methodologies used to address the research objectives
in this thesis. It describes the feature extraction and  engineering methods,
the use of explainable AI methods, model training and optimization approaches,
clustering and dimensionality reduction techniques and statistical hypothesis
testing methods.
%---------------------------------------------------------------------------

\section{Feature Engineering}
\label{sec:featureengineering}

Feature engineering is a critical step in the research process, as it involves
transforming raw data acquired using the data collection script into meaningful
representations that can be analyzed or used for predictive modeling. This
section describes the methodologies used to extract acoustic features from the
mp3 files and lyrical features from the lyrics. These features are designed to
capture key characteristics of the songs, enabling deeper insights into their
patterns and relationships.

\subsection{Acoustic Features}
\label{sec:acousticfeatures}

 These features provide a quantitative representation of the audio properties
 of each song and were extracted directly from the audio files in MP3 format.
 They describe various aspects of audio signal and provide insights into the
 rhythm, timbre, harmony and other acoustic properties. The extraction was done
 using \textit{Librosa} and was automated and parallelized to make it suitable
 for processing large amounts of data. 

\subsubsection*{MFCC - Mel Frequency Cepstral Coefficients}
MFCCs represent the short-term power spectrum of a song on a mel-scale and are
widely used for timbre analysis. These coefficients capture the tonal quality
of the audio and help differentiate between different instruments and vocal
characteristics.


\subsubsection*{Chroma}
Chroma vectors represent the intensity of each pitch class (e.g., C, C\#, D,
etc.) in the audio. These features provide a harmonic representation of the
song and are useful for analyzing chord progressions and harmonic structures.

\subsubsection*{Spectral Contrast}
Spectral contrast measures the difference in amplitude between peaks and
valleys in the spectrum. It provides insights into the harmonic and timbral
content of a song, particularly useful for distinguishing between smooth and
complex textures.

\subsubsection*{Other Features}
Two additional features were extracted:
\begin{itemize}
  \item \textbf{Tempo} - refers to the speed of the song, measured in \textit{Beats Per Minute(BPM)}
  \item \textbf{Zero Crossing Rate(ZCR)} - measures the rate at which the audio
    signal changes sign. It's commonly used as a measure of noisiness or
    percussive nature of signal
\end{itemize}

%---------------------------------------------------------------------------

\subsection{Lyrical Features}
\label{sec:lyricalfeatures}

Lyrical features  were extracted from the lyrics fetched during the data
collection process. They aim to provide a linguistic and semantic
representation of the track, capturing their complexity, sentiment and
stylistic  attributes. The cleaning and extraction  process utilized various
NLP libraries like \textit{NLTK}, \textit{spaCy} and \textit{TextBlob},
alongside with custom ad-hoc algorithms. Similarily to acoustic features the
implementation allowed for simple and intuitive usage under clear and
comprehensible interface, with parallelization of the computation process for
increased performance. The features extracted can be grouped as follows:

\subsubsection*{Basic Linguistic Metrics}
\begin{itemize}
  \item \textbf{Unique Word Count} - measures number of unique words in the lyrics, indicating diversity
  \item \textbf{Type-Token Ratio} - a measure of lexical richness: ratio of unique words to total words
  \item \textbf{Word Count} - total  number of words, baseline for text size and compexity
  \item \textbf{Noun and Verb Ratios} - proporrtios of nous and vers relative to the total word count
\end{itemize}


\subsubsection*{Sentiment and Emotional Tone}
\begin{itemize}
  \item \textbf{Sentiment Polarity} - a measure of overall sentiment(positive
    vs. negative) of the text
  \item \textbf{Sentiment Subjectivity} - represents the degree of subjectivity
    in the lyrics, attempting to make a distinction between factual and
    opinionated content
  \item \textbf{VADER Compound} - a sentiment score derived from the VADER tool
  \item \textbf{Sentiment Variability} - standard deviation of sentiment on
    subsets of lyrics, a metric  aiming to capture fluctuations of sentiment
    throughout the song, highlighting emotional complexity
  \item \textbf{} -
\end{itemize}


\subsubsection*{Stylistic Features}
\begin{itemize}
  \item \textbf{Repetition Count} - the frequency of repeated words
  \item \textbf{Rhyme Density} - a measure of how often rhymes occur in the
    text
  % \item \textbf{Linguistic Uniqueness} - a measure of 
\end{itemize}


\subsubsection*{Semantic and Complexity Features}
\begin{itemize}
  \item \textbf{Semantic Depth} - represents the richness and variety of
    meaning conveyed by the lyrics
  \item \textbf{Syntactic Complexity} - captures the sophistication of
    sentence structures
  \item \textbf{Lexical Richness} - quantifies the variety and richness of the
    vocabulary
\end{itemize}



\subsubsection*{Readability and Accessibility}
\begin{itemize}
  \item \textbf{Flesch Reading Ease} - indicates how easy the lyrics are to
    read
  \item \textbf{Gunning Fog} - a metric that estimates the  years of education
    required to understand the text
  \item \textbf{Dale Chall} - a metric that accounts for familiar and
    unfamiliar words in the text
\end{itemize}


\subsubsection*{Contextual Information}
  In process of feature extraction the \textbf{language} of lyrics was also
  identified using \textit{langdetect} library that uses a classification model
  to make predictions based on n-grams extracted from the text. The identified
  language was also used in the cleaning process, to identify which stemmer and
  stopwords language  to use.


\subsubsection*{TF-IDF (Term Frequency - Inverse Document Frequency)}

TF-IDF\cite{tfidf} is a measure that can quantify the relevance of tokens in a document
amongst a collection of documents. In the context of this study, it determines
how important a word in a song's lyrics is  compared to all other song's lyrics
in the dataset. It's used to highlight words that are unique or meaningful
while giving less importance to very common words like 'the' or 'and'.

It can be broken down into two parts:


\begin{itemize}
  \item \textbf{TF - Term Frequency} - Measures the frequency of a term within
    a document:
  \[
  TF(w, d) = \frac{f_{w, d}}{N_d}
  \]
  where:
  \begin{itemize}
    \item \( f_{w, d} \): The number of times the word \( w \) appears in the document \( d \).
    \item \( N_d \): The total number of words in the document \( d \).
  \end{itemize}

  \item \textbf{IDF - Inverse Document Frequency} - Measures the rarity of a
    term across a collection of documents:
  \[
  IDF(w) = \log{\frac{N}{1 + n_w}}
  \]
  where:
  \begin{itemize}
    \item \( N \): The total number of documents in the corpus.
    \item \( n_w \): The number of documents containing the word \( w \).
  \end{itemize}
\end{itemize}

%---------------------------------------------------------------------------

\section{Explainable AI Methods}
\label{sec:explainableaimethods}


Explainable AI (XAI) techniques provide insights into the decision-making
processes of ML models, making it possible to understand the complex
relationships captured within the training data. By bridging the gap between
the pattern-recognition capabilities of these models and their practical
applications, XAI enables transparency and improves the interpretability of
results.

In this study this methodology was applied in various experiments to understand
how specific variables influence others, with aim of uncovering rrelationships
in the data and validating hypotheses. A deeper  understanding of factors
driving model predictions ensured that the results were both reliable and
meaningfully adressed the research objectives. The  techniques employed in this paper are: 

\subsubsection*{SHAP (SHapley Additive exPlanations)}
SHAP values offer a detailed  breakdown of how individual features contribute
to  each prediction. It uses game theory principles to compute the importance
of each feature for a given output, providing both global insights on general
feature importance and  local explainations, showing how diffrenet features
contributed to specific predictions. In this study SHAP values were used in
order to explain the predictive process of trained Catboost models, allowing
for identification of key features for specific prediction task and
visualization of relationships.

\begin{center}
\begin{figure}[ht]
  \centering
  \includegraphics[width=4in]{img/shap_intro.png}
  \caption{SHAP}
  \label{Figure:fig_beh}
\end{figure}
\end{center}

\begin{center}
\begin{figure}[ht]
  \centering
  \includegraphics[width=4.5in]{img/shap_beeswarm.png}
  \caption{Example SHAP beeswarm plot showing impact of some lyrical features
  on the classifier of \textit{explicitness}}
  \label{Figure:fig_beh}
\end{figure}
\end{center}

\begin{center}
\begin{figure}[ht]
  \centering
  \includegraphics[width=3in]{img/shap_feature_importance.png}
  \caption{Example SHAP feature importance plot showing impact of some lyrical features
  on the classifier of \textit{explicitness}}
  \label{Figure:fig_beh}
\end{figure}
\end{center}
%---------------------------------------------------------------------------

\subsubsection*{Machine Learning Models}
In this study, CatBoost, a widely recognized gradient boosting algorithm known
for its high predictive accuracy and efficiency, was employed to build
classification and regression models. These models utilized a combination of
acoustic, lyrical and metadata features in order to predict the variable of
interest, leveraging Catboost's strengths in handling diverse and complex
datasets. The result models were then subjected to SHAP analysis, in order to
understand their decision process and understand the interactions between
features and the target variable. Catboost was chosen for its flexibility, ease
of use, robust performance, compatibility with SHAP and built-in support for
categorical features,  which eliminated the need for extensive preprocessing.

In order to further optimize the performance of these models, the
hyperparameter tuning library \textit{Optuna} was used. Its efficient
optimization framework allowed for systematic exploration of different sets of
hyperparameter configurations, ensuring the model achieved optimal performance while avoiding
overfitting. 

To address the challenges commonly encountered when training ML models on
complex datasets, following techniques were employed:
\begin{itemize}
  \item \textbf{Cross-validation} - cross-validation was used to reduce the
    risk of overfitting and provide reliable performance metrics. By dividing
    the data into multiple folds, the model was iteratively trained and
    validated on different subsets, therefore ensuring robust evaluation across
    the dataset and improved model's reliability, at the cost of increased
    computational time.
  \item \textbf{Class Weights} - to handle class imbalance in target variable
    labels, CatBoost offers a built-in capability to assign different penalties
    for misclassifications of specific classes. This adjustment improves
    model's ability to make accurate predictions across all classes, instead of
    favouring the majority class.
  \item \textbf{Out of sample evaluation} - model's performance was assessed on
    a separate test dataset that was excluded from the training. This step
    provided a reliable measure of model's ability to generalize on unseen data
    and ensured evaluation metrics  reflected its real predictive performance.
\end{itemize}

\section{Dimensionality Reduction - Principal Component Analysis (PCA)}
\label{sec:dimensionalityreduction}

Principal Component Analysis (PCA) reduces the number of features in large
datasets by transforming them into principal components that retain most of the
original information. It achieves this by converting potentially correlated
variables into a smaller set of less correlated variables, called principal
components, in a way that preserves as much of the original variance as
possible\cite{pca}. PCA is often employed to reduce dataset dimensionality and improve
generalization by reducing noise and redundancy in the data.

In this study, PCA was applied to the TF-IDF vectors derived from song lyrics.
TF-IDF vectors are typically high-dimensional, with thousands of features
representing individual terms across the corpus. Such high-dimensional data can
pose challenges, including increased computational complexity and a higher risk
of overfitting in machine learning models.

The use of PCA on these vectors reduced their dimensionality while preserving
as many significant patterns from the original vectors as possible. This
process improved computational efficiency and the interpretability of the data,
which is particularly important in the context of Explainable AI (XAI)
methodologies.

\begin{center}
\begin{figure}[ht]
  \centering
  \includegraphics[width=4in]{img/pca.png}
  \caption{A scatterplot showing the relationship between PC1 and PC2 when PCA
  is applied to a dataset. PC1 and PC2 axis are perpendicular to each other.\cite{pca}}
  \label{Figure:fig_beh}
\end{figure}
\end{center}


\section{Topic Modelling - Latent Dirichlet Allocation (LDA)}
\label{sec:topicmodelling}

Latent Dirichlet Allocation (LDA)\cite{lda} is a generative probabilistic model
designed to uncover latent topics within a collection of discrete data, such as
text corpus.  It represents each document as a mixture of topics, where each
topic is characterized by a distribution over words. In this study, LDA was
applied to the lyrics dataset to identify prevalent themes and topics across
different songs. By representing each song as a distribution over topics, LDA
provided insights into the thematic content of the lyrics, allowing for the
analysis of how these themes correlate with acoustic features and other song
attributes.

In this study, LDA was applied on the lyrics in order to identify commonly
ocurrirng topics in songs. Each song's lyrics were represented as a combination
of topics, and the most representative words for each topic were extracted.
This provided insights into the thematic content of lyrics, enabling further
exploration of relationships between lyrical topics and other features, like
popularity orr acoustic properties.

By combining the topics derived with LDA with additional features, such as
acoustic parameters, the study aimed to analyze the interplay between a song's
musical and lyrical components.


\section{Statistical Methods}
\label{sec:statisticalmethods}

Statistical methods provided a foundation for analyzing complex relationships
between features and a framework for descriptive data analysis and hypothesis
testing.

\subsection{Person Correlation}

Person Corrrelation is a statistical measure used to quantify linear
relationship between two continuous variables. The resulting coefficient ranges
from -1 to 1 and is computed in the following way:

\[
r = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum{(x_i - \bar{x})^2} \sum{(y_i - \bar{y})^2}}}
\]

Where:
\begin{itemize}
    \item \( x_i \) and \( y_i \): The data points for the two variables.
    \item \( \bar{x} \) and \( \bar{y} \): The mean values of the variables.
\end{itemize}

Interpretation:
\begin{itemize}
  \item An \textit{r} value close to 1 indicates very strong positive linear relationship
  \item An \textit{r} value close to -1 indicates very strong negative linear relationship
  \item An \textit{r} value close to 0 indicates little to no relationship
\end{itemize}



\subsection{Bootstrap Testing}

Bootstrap testing was used to verify hypotheses specified in the research
objectives. It's a resampling-based statistical technique that estimates the
variability of a statistic(e.g. mean or median)  by
repeatedly sampling its values from the dataset with replacement. It's highly
versatile since it doesn't rely on strong distributional assumptions.


\begin{center}
\begin{figure}[ht]
  \centering
  \includegraphics[width=5in]{img/bootstrap.jpg}
  \caption{Illustration of bootstrap resampling: The distribution of the
  statistic (e.g., mean) of the target variable for two different samples
(called groups). This demonstrates the variability of the statistic across
resampled datasets.}
  \label{Figure:fig_beh}
\end{figure}
\end{center}
